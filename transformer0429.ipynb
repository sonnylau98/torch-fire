{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMgeNeg28CEsi9DUYQ7y7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonnylau98/torch-fire/blob/main/transformer0429.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "um_YyUKAF3sR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from matplotlib_inline import backend_inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import inspect\n",
        "import tarfile\n",
        "import zipfile\n",
        "import os\n",
        "import hashlib\n",
        "import requests\n",
        "import collections\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
        "DATA_HUB = dict()"
      ],
      "metadata": {
        "id": "SpTAfhx18F1H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions\n",
        "\n",
        "numpy = lambda x, *args, **kwargs: x.detach().numpy(*args, **kwargs)\n",
        "to = lambda x, *args, **kwargs: x.to(*args, **kwargs)\n",
        "reshape = lambda x, *args, **kwargs: x.reshape(*args, **kwargs)\n",
        "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
        "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\n",
        "reduce_mean = lambda x, *args, **kwargs: x.mean(*args, **kwargs)\n",
        "to = lambda x, *args, **kwargs: x.to(*args, **kwargs)\n",
        "expand_dims = lambda x, *args, **kwargs: x.unsqueeze(*args, **kwargs)\n",
        "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
        "\n",
        "def cpu():\n",
        "    \"\"\"Get the CPU device.\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def gpu(i=0):\n",
        "    \"\"\"Get a GPU device.\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    return torch.device(f'cuda:{i}')\n",
        "\n",
        "def num_gpus_fun():\n",
        "    \"\"\"Get the number of available GPUs.\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    return torch.cuda.device_count()\n",
        "\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\n",
        "\n",
        "    Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "    if num_gpus_fun() >= i + 1:\n",
        "        return gpu(i)\n",
        "    return cpu()\n",
        "\n",
        "def bleu(pred_seq, label_seq, k):\n",
        "    \"\"\"Compute the BLEU.\n",
        "\n",
        "    Defined in :numref:`sec_seq2seq_training`\"\"\"\n",
        "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
        "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
        "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
        "    for n in range(1, min(k, len_pred) + 1):\n",
        "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
        "        for i in range(len_label - n + 1):\n",
        "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
        "        for i in range(len_pred - n + 1):\n",
        "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
        "                num_matches += 1\n",
        "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
        "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
        "    return score"
      ],
      "metadata": {
        "id": "8WIp5JLM3j4f",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFFN(nn.Module):\n",
        "    \"\"\"The positionwise feed-forward network.\"\"\"\n",
        "    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n",
        "        super().__init__()\n",
        "        self.dense1 = nn.LazyLinear(ffn_num_hiddens)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dense2 = nn.LazyLinear(ffn_num_outputs)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.dense2(self.relu(self.dense1(X)))"
      ],
      "metadata": {
        "id": "RIvE0bY3Jb3x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    \"\"\"The residual connection followed by layer normalization.\n",
        "\n",
        "    Defined in :numref:`subsec_positionwise-ffn`\"\"\"\n",
        "    def __init__(self, norm_shape, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(norm_shape)\n",
        "\n",
        "    def forward(self, X, Y):\n",
        "        return self.ln(self.dropout(Y) + X)"
      ],
      "metadata": {
        "id": "va1tY2OeJh3v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_softmax(X, valid_lens):\n",
        "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
        "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
        "    def _sequence_mask(X, valid_len, value=0):\n",
        "        maxlen = X.size(1)\n",
        "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                            device=X.device)[None, :] < valid_len[:, None]\n",
        "        X[~mask] = value\n",
        "        return X\n",
        "\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        # On the last axis, replace masked elements with a very large negative\n",
        "        # value, whose exponentiation outputs 0\n",
        "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
      ],
      "metadata": {
        "id": "pgCr-QgjJcB3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries, Keys, and Values\n",
        "\n",
        "\n",
        "So far all the networks we have reviewed crucially relied on the input being of a well-defned size. For instance, the images in ImageNet are of size $224\\times224$ pixels and CNNs are specifically tuned to this size. Even in natural language processing the input size for RNNs is well defined and fixed. Variable size is addressed by sequentially processing one token at a time, or by specially designed convolution kernels.This approach can lead to significant problems when the input is truly of varying size with varyinglinformation content, such as in Section 10.7 in the transformation of text. In particular, for long sequences it becomes quite difficult to keep track of everything that has already been generated or even viewed by the network.\n",
        "Compare this to databases. In their simplest form they are collections of keys $(k)$ and values $(v)$. For instance, our database $\\mathcal{D}$ might consist of tuples {(\"Zhang\",\"Aston\"),(\"Lipton\",\"Zachary\"),(\"Li\",\"Mu\"),(\"Smola\",\"Alex\"),(\"Hu\"\"Rachel\"),(\"Werness\",\"Brent\")} with the last name being the key and the first name being the value. We can operate on $\\mathcal{D}$, for instance with the exact query $(q)$ for \"Li\" which would return the\n",
        " value \"Mu\". If (\"Li\", \"Mu\") was not a record in $D$, there would be no valid answer. If we also allowed for approximate matches, we would retrieve (\"Lipton\",â€œZachary\") instead.\n",
        "\n",
        "This quite simple and trivial example nonetheless teaches us a number of useful things:\n",
        "* We can design queries $q$ that operate on $(k,v)$ pairs in such a manner as to be valid regardless of the database size.\n",
        "* The same query can receive different answers, according to the contents of the database.\n",
        "* The \"code\" being executed for operating on a large state space (the database) can be quite simple (e.g., exact match, approximate match, top-$k$).\n",
        "* There is no need to compress or simplify the database to make the operations effective.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will cover the specifics of the attention mechanis' application to machine translation later. For now, simply consider the following: denote by $\\mathcal{D}\\stackrel{\\mathrm{def}}{=}\\{(\\mathbf{k}_{1},\\mathbf{v}_{1}),\\ldots(\\mathbf{k}_{m},\\mathbf{v}_{m})\\}$ a database of $m$ tuples of keys and values. Moreover, denote by $\\mathbf{q}$ a query. Then we can define the attention over $\\mathcal{D}$ as\n",
        "\n",
        "$$\\text{Attention}(\\mathbf{q},\\mathcal{D})\\stackrel{\\text{def}}{=}\\sum_{i=1}^m\\alpha(\\mathbf{q},\\mathbf{k}_i)\\mathbf{v}_i,$$\n",
        "\n",
        "where $\\alpha(\\mathbf{q},\\mathbf{k}_i)\\in\\mathbb{R}\\left(i=1,\\ldots,m\\right)$ are scalar attention weights. The operation itself is typically referred to as attention pooling. The name $attention$ derives from the fact that the operation pays particular attention to the terms for which the weight $\\alpha$ is significant (i.e., large). As such, the attention over $\\mathcal{D}$ generates a linear combination of values contained in the database. In fact, this contains the above example as a special case where all but one weight is zero. We have a number of special cases:\n",
        "\n",
        "* The weights $\\alpha(\\mathbf{q},\\mathbf{k}_i)$ are nonnegative. In this case the output of the attention mechanism is contained in the convex cone spanned by the values $\\mathbf{v}_i.$\n",
        "* The weights $\\alpha(\\mathbf{q},\\mathbf{k}_i)$ form a convex combination, i.e, $\\sum_i\\alpha(\\mathbf{q},\\mathbf{k}_i)=1$ and $\\alpha(\\mathbf{q},\\mathbf{k}_i)\\geq0$ for all $i$. This is the most common setting in deep learning.\n",
        "* Exactly one of the weights $\\alpha(\\mathbf{q},\\mathbf{k}_i)$ is $1$, while all others are $0$. This is akin to a traditional database query.\n",
        "* All weights are equal, i.e., $\\alpha(\\mathbf{q},\\mathbf{k}_i)=\\frac1m$ for all $i$. This amounts to averaging across the entire database, also called average pooling in deep learning.\n",
        "\n",
        "A common strategy for ensuring that the weights sum up to $1$ is to normalize them via\n",
        "\n",
        "$$\\alpha(\\mathbf{q},\\mathbf{k}_i)=\\frac{\\alpha(\\mathbf{q},\\mathbf{k}_i)}{\\sum_j\\alpha(\\mathbf{q},\\mathbf{k}_j)}.$$\n",
        "\n",
        "In particular, to ensure that the weights are also nonnegative, one can resort to exponentiation. This means that we can now pick anyfunction $\\alpha(\\mathbf{q},\\mathbf{k})$ and then apply the softmax operation used for multinomial models to it via\n",
        "\n",
        "$$\\alpha(\\mathbf{q},\\mathbf{k}_i)=\\frac{\\exp(\\alpha(\\mathbf{q},\\mathbf{k}_i))}{\\sum_j\\exp(\\alpha(\\mathbf{q},\\mathbf{k}_j))}.$$\n",
        "\n",
        "This operation is readily available in all deep learning frameworks. It is differentiable and its gradient never vanishes, all of which are desirable properties in a model. Note though, the attention mechanism introduced above is not the only option. For instance, we can design a non-differentiable attention model that can be trained using reinforcement learning methods. As one would expect, training such a model is quite complex. Consequently the bulk of modern attention research follows the framework outlined in Fig. We thus focus our exposition on this family of differentiable mechanisms.\n",
        "\n",
        "![qkv](https://d2l.ai/_images/qkv.svg)"
      ],
      "metadata": {
        "id": "nNITgyhf0hqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Pooling by Similarity\n",
        "\n",
        "Now that we have introduced the primary components of the attention mechanism, let's use them in a rather classical setting, namely regression and classification via kernel density estimation. This detour simply provides additional background: it is entirely optional and can be skipped if needed. At their core, Nadaraya- Watson estimators rely on some similarity kernel $\\alpha(\\mathbf{q},\\mathbf{k})$ relating queries $\\mathbf{q}$ to keys $\\mathbf{k}$. Some common kernels are\n",
        "\n",
        "$$\\begin{aligned}\n",
        "&\\alpha(\\mathbf{q,k}) =\\exp\\left(-\\frac12\\|\\mathbf{q}-\\mathbf{k}\\|^2\\right) && \\text{Gaussian;}  \\\\\n",
        "&\\alpha(\\mathbf{q,k}) =1\\mathrm{~if~}\\|\\mathbf{q}-\\mathbf{k}\\|\\leq1 && \\mathrm{Boxcar};  \\\\\n",
        "&\\alpha(\\mathbf{q,k}) =\\max\\left(0,1-\\|\\mathbf{q}-\\mathbf{k}\\|\\right) && \\text{Epanechikov.}\n",
        "\\end{aligned}$$\n",
        "\n",
        "There are many more choices that we could pick. See a Wikipedia article for a more extensive review and how the choice of kernels is related to kernel density estimation, sometimes also called Parzen Windows (Parzen,1957). All of the kernels are heuristic and can be tuned. For instance, we can adjust the width, not only on a global basis but even on a per-coordinate basis. Regardless, all of them lead to the following equation for regression and classification alike:\n",
        "\n",
        "$$f(\\mathbf{q})=\\sum_i\\mathbf{v}_i\\frac{\\alpha(\\mathbf{q},\\mathbf{k}_i)}{\\sum_j\\alpha(\\mathbf{q},\\mathbf{k}_j)}.$$\n",
        "\n",
        "In the case of a (scalar) regression with observations $(\\mathbf{x}_i,y_i)$ for features and labels respectively, $\\mathbf{v}_i=y_i$ are scalars, $\\mathbf{k}_i=\\mathbf{x}_i$ are vectors, and the query $\\mathbf{q}$ denotes the new location where $f$ should be evaluated. In the case of (muticlass) classification, we use one-hot-encoding of $y_i$ to obtain $\\mathbf{v}_i$. One of the convenient properties of this estimator is that it requires no training. Even more so, if we suitably narrow the kernel with increasing amounts of data, the approach is consistent, i.e., it will converge to some statistically optimal solution. Let's start by inspecting some kernels."
      ],
      "metadata": {
        "id": "tbJbcl6F7fBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Matrix Multiplication\n",
        "\n",
        "Another commonly used operation is to multiply batches of matrices by one another. This comes in handy when we have minibatches of queries, keys, and values. More specifically, assume that\n",
        "$$\n",
        "\\mathbf{Q}=[\\mathbf{Q}_1,\\mathbf{Q}_2,\\ldots,\\mathbf{Q}_n]\\in\\mathbb{R}^{n\\times a\\times b},\\\\\\mathbf{K}=[\\mathbf{K}_1,\\mathbf{K}_2,\\ldots,\\mathbf{K}_n]\\in\\mathbb{R}^{n\\times b\\times c}.\n",
        "$$\n",
        "(11.3.4)\n",
        "Then the batch matrix multiplication (BMM) computes the elementwise product\n",
        "\n",
        "$$\n",
        "\\mathrm{BMM}(\\mathbf{Q},\\mathbf{K})=[\\mathbf{Q}_1\\mathbf{K}_1,\\mathbf{Q}_2\\mathbf{K}_2,\\ldots,\\mathbf{Q}_n\\mathbf{K}_n]\\in\\mathbb{R}^{n\\times a\\times c}.\n",
        "$$\n",
        " (11.3.5)\n",
        "Let's see this in action in a deep learning framework."
      ],
      "metadata": {
        "id": "IRIGDQXaWvbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Scoring Functions\n",
        "\n",
        "![attention-output.](https://d2l.ai/_images/attention-output.svg)\n",
        "\n",
        "## Dot Product Attention\n",
        "\n",
        "Let's review the attention function (without exponentiation) from the Gaussian kernel for a moment:\n",
        "\n",
        "$$\n",
        "a(\\mathbf{q},\\mathbf{k}_i)=-\\frac12\\|\\mathbf{q}-\\mathbf{k}_i\\|^2=\\mathbf{q}^\\top\\mathbf{k}_i-\\frac12\\|\\mathbf{k}_i\\|^2-\\frac12\\|\\mathbf{q}\\|^2.\n",
        "$$\n",
        "\n",
        "First, note that the final term depends on $\\mathbf{q}$ only. As such it is identical for all $(\\mathbf{q},\\mathbf{k}_i)$ pairs. Normalizing the attention weights to $1$, as is done in previous section, ensures that this term disappears entirely. Second, note that both batch and layer normalization(to be discussed later) lead to activations that have well-bounded, and often constant, norms $\\|\\mathbf{k}_i\\|$. This is the case, for instance, whenever the keys $\\mathbf{k}_i$ were generated by a layer norm. As such, we can drop it from the definition of $a$ without any major change in the outcome.\n",
        "\n",
        "Last, we need to keep the order of magnitude of the arguments in the exponential function under control. Assume that all the elements of the query $\\mathbf{q}\\in\\mathbb{R}^d$ and the key $\\mathbf{k}_i\\in\\mathbb{R}^d$ are independent and identically drawn random variables with zero mean and unit variance. The dot product between both vectors has zero mean and a variance of $d$. To ensure that the variance of the dot product still remains $1$ regardless of vector length, we use the scaled dot product attention scoring function. Thatis,we rescale the dot product by $1/\\sqrt{d}$. We thus arrive at the first commonly used attention function that is used, e.g., in Transformers:\n",
        "\n",
        "$$\n",
        "a(\\mathbf{q},\\mathbf{k}_i)=\\mathbf{q}^\\top\\mathbf{k}_i/\\sqrt{d}.\n",
        "$$\n",
        "\n",
        "Note that attention weights $\\alpha$ still need normalizing. We can simplify this further via below equation by using the softmax operation:\n",
        "\n",
        "$$\n",
        "\\alpha(\\mathbf{q},\\mathbf{k}_i)=\\mathrm{softmax}(a(\\mathbf{q},\\mathbf{k}_i))=\\frac{\\exp(\\mathbf{q}^\\top\\mathbf{k}_i/\\sqrt{d})}{\\sum_{j=1}\\exp(\\mathbf{q}^\\top\\mathbf{k}_j/\\sqrt{d})}.\n",
        "$$\n",
        "\n",
        "As it turns out, all popular attention mechanisms use the softmax, hence we will limit ourselves to that in the remainder of this chapter."
      ],
      "metadata": {
        "id": "soI60O2WWGiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "  def __init__(self, dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, queries, keys, values, valid_lens=None):\n",
        "    d = queries.shape[-1]\n",
        "    scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
        "    self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "    return torch.bmm(self.dropout(self.attention_weights), values)"
      ],
      "metadata": {
        "id": "8TjyHrREOzKu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Before providing the implementation of multi-head attention, let's formalize this model mathematically. Given a query $\\mathbf{q}\\in\\mathbb{R}^{d_q}$, a key $\\mathbf{k}\\in\\mathbb{R}^{d_k}$, and a value $\\mathbf{v}\\in\\mathbb{R}^{d_v}$, each attention head $\\mathbf{h}_i\\left(i=1,\\ldots,h\\right)$ is computed as\n",
        "\n",
        "$$\n",
        "\\mathbf{h}_i=f(\\mathbf{W}_i^{(q)}\\mathbf{q},\\mathbf{W}_i^{(k)}\\mathbf{k},\\mathbf{W}_i^{(v)}\\mathbf{v})\\in\\mathbb{R}^{p_v},\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W}_i^{(q)}\\in\\mathbb{R}^{p_q\\times d_q},\\mathbf{W}_i^{(k)}\\in\\mathbb{R}^{p_k\\times d_k}$, and $\\mathbf{W}_i^{(v)}\\in\\mathbb{R}^{p_0\\times d_v}$ are learnable parameters and $f$ is attention pooling, such as additive attention and scaled dot product attention in previous section. The multi-head attention output is another linear transformation via learnable parameters\n",
        " $\\mathbf{W}_o\\in\\mathbb{R}^{p_o\\times hp_v}$ of the concatenation of $h$ heads:\n",
        "\n",
        "$$\n",
        "\\mathbf{W}_o\\begin{bmatrix}\\mathbf{h}_1\\\\\\vdots\\\\\\mathbf{h}_h\\end{bmatrix}\\in\\mathbb{R}^{p_o}.\n",
        "$$\n",
        "\n",
        "Based on this design, each head may attend to different parts of the input. More sophisticated functions than the simple weighted average can be expressed.\n",
        "\n"
      ],
      "metadata": {
        "id": "DgejhfGtXECG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![multi-head-attention](https://d2l.ai/_images/multi-head-attention.svg)"
      ],
      "metadata": {
        "id": "HQK9zSfZR38d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_hiddens,\n",
        "               num_heads, dropout, bias=False, **kwargs):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = DotProductAttention(dropout)\n",
        "    self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "    self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "    self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "    self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "\n",
        "  def transpose_qkv(self, X):\n",
        "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
        "\n",
        "  def transpose_output(self, X):\n",
        "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
        "\n",
        "  def forward(self, queries, keys, values, valid_lens):\n",
        "    queries = self.transpose_qkv(self.W_q(queries))\n",
        "    keys = self.transpose_qkv(self.W_k(keys))\n",
        "    values = self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "    if valid_lens is not None:\n",
        "        valid_lens = torch.repeat_interleave(\n",
        "            valid_lens, repeats=self.num_heads, dim=0)\n",
        "\n",
        "    output = self.attention(queries, keys, values, valid_lens)\n",
        "    output_concat = self.transpose_output(output)\n",
        "    return self.W_o(output_concat)"
      ],
      "metadata": {
        "id": "ACjWN0bkPdWT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"The Transformer encoder block.\"\"\"\n",
        "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,\n",
        "                 use_bias=False):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(num_hiddens, num_heads,\n",
        "                                                dropout, use_bias)\n",
        "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
        "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
        "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
        "\n",
        "    def forward(self, X, valid_lens):\n",
        "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
        "        return self.addnorm2(Y, self.ffn(Y))"
      ],
      "metadata": {
        "id": "GVK__SG3JX6L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attention and Positional Encoding\n",
        "\n",
        "![cnn-rnn-self-attention](https://d2l.ai/_images/cnn-rnn-self-attention.svg)\n",
        "\n",
        "Unlike RNNs, which recurrently process tokens of a sequence one-by-one, self-attention ditches sequential operations in favor of parallel computation. Note that self-attention by itself does not preserve the order of the sequence. What do we do if it really matters that the model\n",
        " knows in which order the input sequence arrived?\n",
        " Fhe dominant approach for preserving information about the order of tokens is to represent this to the model as an additional input associated\n",
        " with each token. These inputs are called positional encodings, and they can either be learned or fixed *a priori*. We now describe a simple scheme for fixed positional encodings based on sine and cosine functions.\n",
        "\n",
        "Suppose that the input representation $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ contains the $d$-dimensional embeddings for $n$ tokens of a sequence. The positional encoding outputs $\\mathbf{X}+\\mathbf{P}$ using a positional embedding matrix $\\mathbf{P}\\in\\mathbb{R}^{n\\times d}$ of the same shape, whose element on the $i^\\mathrm{th}$ row and the $(2j)^\\mathrm{th}$ or the $(2j+1)^\\mathrm{th}$\n",
        " column is\n",
        "\n",
        "$$\n",
        "p_{i,2j} =\\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\n",
        "$$\n",
        "\n",
        "$$\n",
        "p_{i,2j+1} =\\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\n",
        "$$\n",
        "\n",
        "At first glance, this trigonometric function design looks weird. Before we give explanations of this design, let's first implement it in the following PositionalEncoding class.\n"
      ],
      "metadata": {
        "id": "yUU040Q1iRE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding.\"\"\"\n",
        "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a long enough P\n",
        "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
        "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
        "            -1, 1) / torch.pow(10000, torch.arange(\n",
        "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
        "        self.P[:, :, 0::2] = torch.sin(X)\n",
        "        self.P[:, :, 1::2] = torch.cos(X)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
        "        return self.dropout(X)"
      ],
      "metadata": {
        "id": "bq_qSloMhzmm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Transformer Architecture\n",
        "\n",
        "![transformer](http://d2l.ai/_images/transformer.svg)"
      ],
      "metadata": {
        "id": "RTI3j62t_VSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"The Transformer encoder.\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
        "                 num_heads, num_blks, dropout, use_bias=False):\n",
        "        super().__init__()\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
        "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_blks):\n",
        "            self.blks.add_module(\"block\"+str(i), TransformerEncoderBlock(\n",
        "                num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
        "\n",
        "    def forward(self, X, valid_lens):\n",
        "        # Since positional encoding values are between -1 and 1, the embedding\n",
        "        # values are multiplied by the square root of the embedding dimension\n",
        "        # to rescale before they are summed up\n",
        "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
        "        self.attention_weights = [None] * len(self.blks)\n",
        "        for i, blk in enumerate(self.blks):\n",
        "            X = blk(X, valid_lens)\n",
        "            self.attention_weights[\n",
        "                i] = blk.attention.attention.attention_weights\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "4m0RpjFdggc4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    # The i-th block in the Transformer decoder\n",
        "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\n",
        "        super().__init__()\n",
        "        self.i = i\n",
        "        self.attention1 = MultiHeadAttention(num_hiddens, num_heads,\n",
        "                                                 dropout)\n",
        "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
        "        self.attention2 = MultiHeadAttention(num_hiddens, num_heads,\n",
        "                                                 dropout)\n",
        "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
        "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
        "        self.addnorm3 = AddNorm(num_hiddens, dropout)\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
        "        # During training, all the tokens of any output sequence are processed\n",
        "        # at the same time, so state[2][self.i] is None as initialized. When\n",
        "        # decoding any output sequence token by token during prediction,\n",
        "        # state[2][self.i] contains representations of the decoded output at\n",
        "        # the i-th block up to the current time step\n",
        "        if state[2][self.i] is None:\n",
        "            key_values = X\n",
        "        else:\n",
        "            key_values = torch.cat((state[2][self.i], X), dim=1)\n",
        "        state[2][self.i] = key_values\n",
        "        if self.training:\n",
        "            batch_size, num_steps, _ = X.shape\n",
        "            # Shape of dec_valid_lens: (batch_size, num_steps), where every\n",
        "            # row is [1, 2, ..., num_steps]\n",
        "            dec_valid_lens = torch.arange(\n",
        "                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
        "        else:\n",
        "            dec_valid_lens = None\n",
        "        # Self-attention\n",
        "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
        "        Y = self.addnorm1(X, X2)\n",
        "        # Encoder-decoder attention. Shape of enc_outputs:\n",
        "        # (batch_size, num_steps, num_hiddens)\n",
        "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
        "        Z = self.addnorm2(Y, Y2)\n",
        "        return self.addnorm3(Z, self.ffn(Z)), state\n"
      ],
      "metadata": {
        "id": "r-PzE6KLeptO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
        "                 num_blks, dropout):\n",
        "        super().__init__()\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.num_blks = num_blks\n",
        "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
        "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_blks):\n",
        "            self.blks.add_module(\"block\"+str(i), TransformerDecoderBlock(\n",
        "                num_hiddens, ffn_num_hiddens, num_heads, dropout, i))\n",
        "        self.dense = nn.LazyLinear(vocab_size)\n",
        "\n",
        "    def init_state(self, enc_outputs, enc_valid_lens):\n",
        "        return [enc_outputs, enc_valid_lens, [None] * self.num_blks]\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
        "        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
        "        for i, blk in enumerate(self.blks):\n",
        "            X, state = blk(X, state)\n",
        "            # Decoder self-attention weights\n",
        "            self._attention_weights[0][\n",
        "                i] = blk.attention1.attention.attention_weights\n",
        "            # Encoder-decoder attention weights\n",
        "            self._attention_weights[1][\n",
        "                i] = blk.attention2.attention.attention_weights\n",
        "        return self.dense(X), state\n",
        "\n",
        "    @property\n",
        "    def attention_weights(self):\n",
        "        return self._attention_weights\n"
      ],
      "metadata": {
        "id": "9jKpO_0ckfZf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperParameters:\n",
        "    \"\"\"The base class of hyperparameters.\"\"\"\n",
        "    def save_hyperparameters(self, ignore=[]):\n",
        "        \"\"\"Defined in :numref:`sec_oo-design`\"\"\"\n",
        "        raise NotImplemented\n",
        "\n",
        "    def save_hyperparameters(self, ignore=[]):\n",
        "        \"\"\"Save function arguments into class attributes.\n",
        "\n",
        "        Defined in :numref:`sec_utils`\"\"\"\n",
        "        frame = inspect.currentframe().f_back\n",
        "        _, _, _, local_vars = inspect.getargvalues(frame)\n",
        "        self.hparams = {k:v for k, v in local_vars.items()\n",
        "                        if k not in set(ignore+['self']) and not k.startswith('_')}\n",
        "        for k, v in self.hparams.items():\n",
        "            setattr(self, k, v)"
      ],
      "metadata": {
        "id": "IRkJeZiyz-yc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgressBoard(HyperParameters):\n",
        "    \"\"\"The board that plots data points in animation.\n",
        "\n",
        "    Defined in :numref:`sec_oo-design`\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 ls=['-', '--', '-.', ':'], colors=['C0', 'C1', 'C2', 'C3'],\n",
        "                 fig=None, axes=None, figsize=(3.5, 2.5), display=True):\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def draw(self, x, y, label, every_n=1):\n",
        "        raise NotImplemented\n",
        "\n",
        "    def draw(self, x, y, label, every_n=1):\n",
        "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
        "        Point = collections.namedtuple('Point', ['x', 'y'])\n",
        "        if not hasattr(self, 'raw_points'):\n",
        "            self.raw_points = collections.OrderedDict()\n",
        "            self.data = collections.OrderedDict()\n",
        "        if label not in self.raw_points:\n",
        "            self.raw_points[label] = []\n",
        "            self.data[label] = []\n",
        "        points = self.raw_points[label]\n",
        "        line = self.data[label]\n",
        "        points.append(Point(x, y))\n",
        "        if len(points) != every_n:\n",
        "            return\n",
        "        mean = lambda x: sum(x) / len(x)\n",
        "        line.append(Point(mean([p.x for p in points]),\n",
        "                          mean([p.y for p in points])))\n",
        "        points.clear()\n",
        "        if not self.display:\n",
        "            return\n",
        "        use_svg_display()\n",
        "        if self.fig is None:\n",
        "            self.fig = plt.figure(figsize=self.figsize)\n",
        "        plt_lines, labels = [], []\n",
        "        for (k, v), ls, color in zip(self.data.items(), self.ls, self.colors):\n",
        "            plt_lines.append(plt.plot([p.x for p in v], [p.y for p in v],\n",
        "                                          linestyle=ls, color=color)[0])\n",
        "            labels.append(k)\n",
        "        axes = self.axes if self.axes else plt.gca()\n",
        "        if self.xlim: axes.set_xlim(self.xlim)\n",
        "        if self.ylim: axes.set_ylim(self.ylim)\n",
        "        if not self.xlabel: self.xlabel = self.x\n",
        "        axes.set_xlabel(self.xlabel)\n",
        "        axes.set_ylabel(self.ylabel)\n",
        "        axes.set_xscale(self.xscale)\n",
        "        axes.set_yscale(self.yscale)\n",
        "        axes.legend(plt_lines, labels)\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)"
      ],
      "metadata": {
        "id": "F7MxX0qp0Z2m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Module(nn.Module, HyperParameters):\n",
        "    \"\"\"The base class of models.\n",
        "\n",
        "    Defined in :numref:`sec_oo-design`\"\"\"\n",
        "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.board = ProgressBoard()\n",
        "\n",
        "    def loss(self, y_hat, y):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, X):\n",
        "        assert hasattr(self, 'net'), 'Neural network is defined'\n",
        "        return self.net(X)\n",
        "\n",
        "    def plot(self, key, value, train):\n",
        "        \"\"\"Plot a point in animation.\"\"\"\n",
        "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
        "        self.board.xlabel = 'epoch'\n",
        "        if train:\n",
        "            x = self.trainer.train_batch_idx / \\\n",
        "                self.trainer.num_train_batches\n",
        "            n = self.trainer.num_train_batches / \\\n",
        "                self.plot_train_per_epoch\n",
        "        else:\n",
        "            x = self.trainer.epoch + 1\n",
        "            n = self.trainer.num_val_batches / \\\n",
        "                self.plot_valid_per_epoch\n",
        "        self.board.draw(x, numpy(to(value, cpu())),\n",
        "                        ('train_' if train else 'val_') + key,\n",
        "                        every_n=int(n))\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        self.plot('loss', l, train=True)\n",
        "        return l\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        self.plot('loss', l, train=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Defined in :numref:`sec_classification`\"\"\"\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def apply_init(self, inputs, init=None):\n",
        "        \"\"\"Defined in :numref:`sec_lazy_init`\"\"\"\n",
        "        self.forward(*inputs)\n",
        "        if init is not None:\n",
        "            self.net.apply(init)"
      ],
      "metadata": {
        "id": "F6pkhBaJ0PR_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(Module):\n",
        "    \"\"\"The base class of classification models.\n",
        "\n",
        "    Defined in :numref:`sec_classification`\"\"\"\n",
        "    def validation_step(self, batch):\n",
        "        Y_hat = self(*batch[:-1])\n",
        "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
        "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\n",
        "\n",
        "    def accuracy(self, Y_hat, Y, averaged=True):\n",
        "        \"\"\"Compute the number of correct predictions.\n",
        "\n",
        "        Defined in :numref:`sec_classification`\"\"\"\n",
        "        Y_hat = reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
        "        preds = astype(argmax(Y_hat, axis=1), Y.dtype)\n",
        "        compare = astype(preds == reshape(Y, -1), torch.float32)\n",
        "        return reduce_mean(compare) if averaged else compare\n",
        "\n",
        "    def loss(self, Y_hat, Y, averaged=True):\n",
        "        \"\"\"Defined in :numref:`sec_softmax_concise`\"\"\"\n",
        "        Y_hat = reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
        "        Y = reshape(Y, (-1,))\n",
        "        return torch.nn.functional.cross_entropy(\n",
        "            Y_hat, Y, reduction='mean' if averaged else 'none')\n",
        "\n",
        "    def layer_summary(self, X_shape):\n",
        "        \"\"\"Defined in :numref:`sec_lenet`\"\"\"\n",
        "        X = torch.randn(*X_shape)\n",
        "        for layer in self.net:\n",
        "            X = layer(X)\n",
        "            print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
      ],
      "metadata": {
        "id": "Ulc9vOrWmo0H"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(Classifier):\n",
        "    \"\"\"The base class for the encoder--decoder architecture.\n",
        "\n",
        "    Defined in :numref:`sec_encoder-decoder`\"\"\"\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, enc_X, dec_X, *args):\n",
        "        enc_all_outputs = self.encoder(enc_X, *args)\n",
        "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
        "        # Return decoder output only\n",
        "        return self.decoder(dec_X, dec_state)[0]\n",
        "\n",
        "    def predict_step(self, batch, device, num_steps,\n",
        "                     save_attention_weights=False):\n",
        "        \"\"\"Defined in :numref:`sec_seq2seq_training`\"\"\"\n",
        "        batch = [to(a, device) for a in batch]\n",
        "        src, tgt, src_valid_len, _ = batch\n",
        "        enc_all_outputs = self.encoder(src, src_valid_len)\n",
        "        dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n",
        "        outputs, attention_weights = [expand_dims(tgt[:, 0], 1), ], []\n",
        "        for _ in range(num_steps):\n",
        "            Y, dec_state = self.decoder(outputs[-1], dec_state)\n",
        "            outputs.append(argmax(Y, 2))\n",
        "            # Save attention weights (to be covered later)\n",
        "            if save_attention_weights:\n",
        "                attention_weights.append(self.decoder.attention_weights)\n",
        "        return torch.cat(outputs[1:], 1), attention_weights"
      ],
      "metadata": {
        "id": "VIfGT8jHoVIG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(EncoderDecoder):\n",
        "    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\n",
        "\n",
        "    Defined in :numref:`sec_seq2seq_decoder`\"\"\"\n",
        "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        Y_hat = self(*batch[:-1])\n",
        "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Adam optimizer is used here\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
      ],
      "metadata": {
        "id": "96ShTyWiowgu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download(url, folder='../data', sha1_hash=None):\n",
        "    \"\"\"Download a file to folder and return the local filepath.\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    if not url.startswith('http'):\n",
        "        # For back compatability\n",
        "        url, sha1_hash = DATA_HUB[url]\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    fname = os.path.join(folder, url.split('/')[-1])\n",
        "    # Check if hit cache\n",
        "    if os.path.exists(fname) and sha1_hash:\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname\n",
        "    # Download\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "def extract(filename, folder=None):\n",
        "    \"\"\"Extract a zip/tar file into folder.\n",
        "\n",
        "    Defined in :numref:`sec_utils`\"\"\"\n",
        "    base_dir = os.path.dirname(filename)\n",
        "    _, ext = os.path.splitext(filename)\n",
        "    assert ext in ('.zip', '.tar', '.gz'), 'Only support zip/tar files.'\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(filename, 'r')\n",
        "    else:\n",
        "        fp = tarfile.open(filename, 'r')\n",
        "    if folder is None:\n",
        "        folder = base_dir\n",
        "    fp.extractall(folder)"
      ],
      "metadata": {
        "id": "B7SeoaD-vvLe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataModule(HyperParameters):\n",
        "    \"\"\"The base class of data.\n",
        "\n",
        "    Defined in :numref:`subsec_oo-design-models`\"\"\"\n",
        "    def __init__(self, root='../data', num_workers=4):\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.get_dataloader(train=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.get_dataloader(train=False)\n",
        "\n",
        "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
        "        \"\"\"Defined in :numref:`sec_synthetic-regression-data`\"\"\"\n",
        "        tensors = tuple(a[indices] for a in tensors)\n",
        "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
        "        return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
        "                                           shuffle=train)"
      ],
      "metadata": {
        "id": "WkmHJx6jvjJy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = collections.Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The list of unique tokens\n",
        "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
        "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['<unk>']"
      ],
      "metadata": {
        "id": "VK7rZ9FZxFr3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def use_svg_display():\n",
        "    \"\"\"Use the svg format to display a plot in Jupyter.\n",
        "\n",
        "    Defined in :numref:`sec_calculus`\"\"\"\n",
        "    backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "def set_figsize(figsize=(3.5, 2.5)):\n",
        "    \"\"\"Set the figure size for matplotlib.\n",
        "\n",
        "    Defined in :numref:`sec_calculus`\"\"\"\n",
        "    use_svg_display()\n",
        "    plt.rcParams['figure.figsize'] = figsize"
      ],
      "metadata": {
        "id": "pVMcbrb9xiW3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MTFraEng(DataModule):\n",
        "    \"\"\"The English-French dataset.\n",
        "\n",
        "    Defined in :numref:`sec_machine_translation`\"\"\"\n",
        "    def _download(self):\n",
        "        extract(download(\n",
        "            DATA_URL+'fra-eng.zip', self.root,\n",
        "            '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
        "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "\n",
        "    def _preprocess(self, text):\n",
        "        \"\"\"Defined in :numref:`sec_machine_translation`\"\"\"\n",
        "        # Replace non-breaking space with space\n",
        "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
        "        # Insert space between words and punctuation marks\n",
        "        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
        "        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
        "               for i, char in enumerate(text.lower())]\n",
        "        return ''.join(out)\n",
        "\n",
        "    def _tokenize(self, text, max_examples=None):\n",
        "        \"\"\"Defined in :numref:`sec_machine_translation`\"\"\"\n",
        "        src, tgt = [], []\n",
        "        for i, line in enumerate(text.split('\\n')):\n",
        "            if max_examples and i > max_examples: break\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                # Skip empty tokens\n",
        "                src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
        "                tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
        "        return src, tgt\n",
        "\n",
        "    def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n",
        "        \"\"\"Defined in :numref:`sec_machine_translation`\"\"\"\n",
        "        super(MTFraEng, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
        "            self._download())\n",
        "\n",
        "    def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
        "        \"\"\"Defined in :numref:`subsec_loading-seq-fixed-len`\"\"\"\n",
        "        def _build_array(sentences, vocab, is_tgt=False):\n",
        "            pad_or_trim = lambda seq, t: (\n",
        "                seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
        "            sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
        "            if is_tgt:\n",
        "                sentences = [['<bos>'] + s for s in sentences]\n",
        "            if vocab is None:\n",
        "                vocab = Vocab(sentences, min_freq=2)\n",
        "            array = torch.tensor([vocab[s] for s in sentences])\n",
        "            valid_len = reduce_sum(\n",
        "                astype(array != vocab['<pad>'], torch.int32), 1)\n",
        "            return array, vocab, valid_len\n",
        "        src, tgt = self._tokenize(self._preprocess(raw_text),\n",
        "                                  self.num_train + self.num_val)\n",
        "        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
        "        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
        "        return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
        "                src_vocab, tgt_vocab)\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        \"\"\"Defined in :numref:`subsec_loading-seq-fixed-len`\"\"\"\n",
        "        idx = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
        "        return self.get_tensorloader(self.arrays, train, idx)\n",
        "\n",
        "    def build(self, src_sentences, tgt_sentences):\n",
        "        \"\"\"Defined in :numref:`subsec_loading-seq-fixed-len`\"\"\"\n",
        "        raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
        "            src_sentences, tgt_sentences)])\n",
        "        arrays, _, _ = self._build_arrays(\n",
        "            raw_text, self.src_vocab, self.tgt_vocab)\n",
        "        return arrays\n",
        "\n",
        "def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n",
        "    \"\"\"Plot the histogram for list length pairs.\n",
        "\n",
        "    Defined in :numref:`sec_machine_translation`\"\"\"\n",
        "    set_figsize()\n",
        "    _, _, patches = plt.hist(\n",
        "        [[len(l) for l in xlist], [len(l) for l in ylist]])\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    for patch in patches[1].patches:\n",
        "        patch.set_hatch('/')\n",
        "    plt.legend(legend)"
      ],
      "metadata": {
        "id": "0AQhRltSuvSQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(HyperParameters):\n",
        "    \"\"\"The base class for training models with data.\n",
        "\n",
        "    Defined in :numref:`subsec_oo-design-models`\"\"\"\n",
        "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
        "        self.save_hyperparameters()\n",
        "        assert num_gpus == 0, 'No GPU support yet'\n",
        "\n",
        "    def prepare_data(self, data):\n",
        "        self.train_dataloader = data.train_dataloader()\n",
        "        self.val_dataloader = data.val_dataloader()\n",
        "        self.num_train_batches = len(self.train_dataloader)\n",
        "        self.num_val_batches = (len(self.val_dataloader)\n",
        "                                if self.val_dataloader is not None else 0)\n",
        "\n",
        "    def prepare_model(self, model):\n",
        "        model.trainer = self\n",
        "        model.board.xlim = [0, self.max_epochs]\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self, model, data):\n",
        "        self.prepare_data(data)\n",
        "        self.prepare_model(model)\n",
        "        self.optim = model.configure_optimizers()\n",
        "        self.epoch = 0\n",
        "        self.train_batch_idx = 0\n",
        "        self.val_batch_idx = 0\n",
        "        for self.epoch in range(self.max_epochs):\n",
        "            self.fit_epoch()\n",
        "\n",
        "    def fit_epoch(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def prepare_batch(self, batch):\n",
        "        \"\"\"Defined in :numref:`sec_linear_scratch`\"\"\"\n",
        "        return batch\n",
        "\n",
        "    def fit_epoch(self):\n",
        "        \"\"\"Defined in :numref:`sec_linear_scratch`\"\"\"\n",
        "        self.model.train()\n",
        "        for batch in self.train_dataloader:\n",
        "            loss = self.model.training_step(self.prepare_batch(batch))\n",
        "            self.optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                loss.backward()\n",
        "                if self.gradient_clip_val > 0:  # To be discussed later\n",
        "                    self.clip_gradients(self.gradient_clip_val, self.model)\n",
        "                self.optim.step()\n",
        "            self.train_batch_idx += 1\n",
        "        if self.val_dataloader is None:\n",
        "            return\n",
        "        self.model.eval()\n",
        "        for batch in self.val_dataloader:\n",
        "            with torch.no_grad():\n",
        "                self.model.validation_step(self.prepare_batch(batch))\n",
        "            self.val_batch_idx += 1\n",
        "\n",
        "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
        "        \"\"\"Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "        self.save_hyperparameters()\n",
        "        self.gpus = [gpu(i) for i in range(min(num_gpus, num_gpus_fun()))]\n",
        "\n",
        "\n",
        "    def prepare_batch(self, batch):\n",
        "        \"\"\"Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "        if self.gpus:\n",
        "            batch = [to(a, self.gpus[0]) for a in batch]\n",
        "        return batch\n",
        "\n",
        "\n",
        "    def prepare_model(self, model):\n",
        "        \"\"\"Defined in :numref:`sec_use_gpu`\"\"\"\n",
        "        model.trainer = self\n",
        "        model.board.xlim = [0, self.max_epochs]\n",
        "        if self.gpus:\n",
        "            model.to(self.gpus[0])\n",
        "        self.model = model\n",
        "\n",
        "    def clip_gradients(self, grad_clip_val, model):\n",
        "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
        "        params = [p for p in model.parameters() if p.requires_grad]\n",
        "        norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "        if norm > grad_clip_val:\n",
        "            for param in params:\n",
        "                param.grad[:] *= grad_clip_val / norm"
      ],
      "metadata": {
        "id": "-Bf27edwyo8W"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train\n",
        "\n",
        "data = MTFraEng(batch_size=128)\n",
        "num_hiddens, num_blks, dropout = 256, 2, 0.2\n",
        "ffn_num_hiddens, num_heads = 64, 4\n",
        "encoder = TransformerEncoder(\n",
        "    len(data.src_vocab), num_hiddens, ffn_num_hiddens, num_heads,\n",
        "    num_blks, dropout)\n",
        "decoder = TransformerDecoder(\n",
        "    len(data.tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads,\n",
        "    num_blks, dropout)\n",
        "model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n",
        "                    lr=0.001)\n",
        "trainer = Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "OU6z5Ypq6xuY",
        "outputId": "6cd5f210-3041-44d0-d15d-a69dbdaa642f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"229.425pt\" height=\"183.35625pt\" viewBox=\"0 0 229.425 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-04-29T03:05:44.115470</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 229.425 183.35625 \nL 229.425 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 145.8 \nL 215.8625 145.8 \nL 215.8625 7.2 \nL 20.5625 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m0ee718fae9\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m0ee718fae9\" x=\"20.5625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(17.38125 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m0ee718fae9\" x=\"53.1125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(49.93125 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m0ee718fae9\" x=\"85.6625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(79.3 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m0ee718fae9\" x=\"118.2125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(111.85 160.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m0ee718fae9\" x=\"150.7625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(144.4 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m0ee718fae9\" x=\"183.3125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(176.95 160.398438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m0ee718fae9\" x=\"215.8625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(209.5 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- epoch -->\n     <g transform=\"translate(102.984375 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path id=\"m6efc91263c\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m6efc91263c\" x=\"20.5625\" y=\"145.133694\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 148.932913) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m6efc91263c\" x=\"20.5625\" y=\"115.050113\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1 -->\n      <g transform=\"translate(7.2 118.849332) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m6efc91263c\" x=\"20.5625\" y=\"84.966532\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 88.765751) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m6efc91263c\" x=\"20.5625\" y=\"54.882952\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 58.68217) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m6efc91263c\" x=\"20.5625\" y=\"24.799371\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 28.59859) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 21.37625 13.5 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 27.0725 79.982151 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 27.0725 79.982151 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 27.0725 79.982151 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_51\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_52\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_53\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_54\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_55\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_56\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_57\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_58\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_59\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_60\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_61\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_62\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_72\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_73\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_74\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_75\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_76\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_79\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_80\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_81\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_82\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_83\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_84\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_85\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_86\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_87\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_94\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_95\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_96\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_97\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_98\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_99\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_100\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_101\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_102\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_103\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_104\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_105\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_106\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_107\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_108\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_109\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_110\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_111\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_112\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_113\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_114\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_115\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_116\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_117\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_118\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_119\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_120\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_121\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_122\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_123\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_124\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_125\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_126\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_127\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_128\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_129\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_130\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_131\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_132\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_133\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_134\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_135\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_136\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_137\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_138\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_139\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_140\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_141\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_142\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_143\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_144\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_145\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_146\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_147\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_148\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_149\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_150\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_151\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_152\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_153\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_154\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_155\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_156\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_157\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_158\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_159\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_160\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_161\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_162\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_163\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_164\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_165\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_166\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_167\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_168\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_169\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_170\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_171\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_172\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_173\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \nL 197.14625 139.129039 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_174\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_175\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \nL 197.14625 139.129039 \nL 200.40125 139.043019 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_176\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_177\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \nL 197.14625 139.129039 \nL 200.40125 139.043019 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_178\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \nL 202.8425 95.005187 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_179\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \nL 197.14625 139.129039 \nL 200.40125 139.043019 \nL 203.65625 139.366488 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_180\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \nL 202.8425 95.005187 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_181\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \nL 197.14625 139.129039 \nL 200.40125 139.043019 \nL 203.65625 139.366488 \nL 206.91125 139.219853 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_182\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \nL 202.8425 95.005187 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_183\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \nL 197.14625 139.129039 \nL 200.40125 139.043019 \nL 203.65625 139.366488 \nL 206.91125 139.219853 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_184\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \nL 202.8425 95.005187 \nL 209.3525 95.617857 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_185\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \nL 197.14625 139.129039 \nL 200.40125 139.043019 \nL 203.65625 139.366488 \nL 206.91125 139.219853 \nL 210.16625 138.81844 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_186\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \nL 202.8425 95.005187 \nL 209.3525 95.617857 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_187\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \nL 197.14625 139.129039 \nL 200.40125 139.043019 \nL 203.65625 139.366488 \nL 206.91125 139.219853 \nL 210.16625 138.81844 \nL 213.42125 139.5 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_188\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \nL 202.8425 95.005187 \nL 209.3525 95.617857 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_189\">\n    <path d=\"M 21.37625 13.5 \nL 24.63125 73.753669 \nL 27.88625 91.001059 \nL 31.14125 100.432624 \nL 34.39625 106.313772 \nL 37.65125 110.024956 \nL 40.90625 113.154849 \nL 44.16125 111.896947 \nL 47.41625 115.720186 \nL 50.67125 115.754713 \nL 53.92625 117.055691 \nL 57.18125 119.776524 \nL 60.43625 121.421229 \nL 63.69125 119.969674 \nL 66.94625 122.130942 \nL 70.20125 123.141563 \nL 73.45625 125.620691 \nL 76.71125 123.128508 \nL 79.96625 125.321387 \nL 83.22125 126.298748 \nL 86.47625 128.17719 \nL 89.73125 126.65285 \nL 92.98625 129.403938 \nL 96.24125 127.959747 \nL 99.49625 130.772109 \nL 102.75125 129.367088 \nL 106.00625 130.295697 \nL 109.26125 131.143138 \nL 112.51625 132.356343 \nL 115.77125 131.484886 \nL 119.02625 132.80675 \nL 122.28125 133.228685 \nL 125.53625 132.774121 \nL 128.79125 134.296144 \nL 132.04625 134.684677 \nL 135.30125 134.411585 \nL 138.55625 135.496955 \nL 141.81125 134.491795 \nL 145.06625 135.482518 \nL 148.32125 136.213717 \nL 151.57625 136.0754 \nL 154.83125 136.34686 \nL 158.08625 137.100051 \nL 161.34125 136.562443 \nL 164.59625 137.070117 \nL 167.85125 137.294695 \nL 171.10625 137.782681 \nL 174.36125 137.953932 \nL 177.61625 138.249574 \nL 180.87125 138.163938 \nL 184.12625 137.964902 \nL 187.38125 138.527701 \nL 190.63625 139.071328 \nL 193.89125 138.230974 \nL 197.14625 139.129039 \nL 200.40125 139.043019 \nL 203.65625 139.366488 \nL 206.91125 139.219853 \nL 210.16625 138.81844 \nL 213.42125 139.5 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_190\">\n    <path d=\"M 27.0725 79.982151 \nL 33.5825 87.655622 \nL 40.0925 90.711933 \nL 46.6025 88.929155 \nL 53.1125 90.294735 \nL 59.6225 83.483822 \nL 66.1325 89.334796 \nL 72.6425 94.147523 \nL 79.1525 89.641477 \nL 85.6625 93.963753 \nL 92.1725 93.316194 \nL 98.6825 95.380901 \nL 105.1925 94.725489 \nL 111.7025 94.343256 \nL 118.2125 94.668021 \nL 124.7225 93.728567 \nL 131.2325 92.997651 \nL 137.7425 95.92253 \nL 144.2525 96.049825 \nL 150.7625 95.460186 \nL 157.2725 96.01201 \nL 163.7825 94.759839 \nL 170.2925 97.197987 \nL 176.8025 96.841171 \nL 183.3125 96.172973 \nL 189.8225 94.669941 \nL 196.3325 93.909877 \nL 202.8425 95.005187 \nL 209.3525 95.617857 \nL 215.8625 95.416077 \n\" clip-path=\"url(#pb9f0321bb8)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 145.8 \nL 20.5625 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 215.8625 145.8 \nL 215.8625 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 145.8 \nL 215.8625 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 7.2 \nL 215.8625 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 129.271875 45.1125 \nL 208.8625 45.1125 \nQ 210.8625 45.1125 210.8625 43.1125 \nL 210.8625 14.2 \nQ 210.8625 12.2 208.8625 12.2 \nL 129.271875 12.2 \nQ 127.271875 12.2 127.271875 14.2 \nL 127.271875 43.1125 \nQ 127.271875 45.1125 129.271875 45.1125 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_191\">\n     <path d=\"M 131.271875 20.298438 \nL 141.271875 20.298438 \nL 151.271875 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- train_loss -->\n     <g transform=\"translate(159.271875 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n     </g>\n    </g>\n    <g id=\"line2d_192\">\n     <path d=\"M 131.271875 35.254688 \nL 141.271875 35.254688 \nL 151.271875 35.254688 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- val_loss -->\n     <g transform=\"translate(159.271875 38.754688) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb9f0321bb8\">\n   <rect x=\"20.5625\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test\n",
        "\n",
        "engs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\n",
        "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
        "preds, _ = model.predict_step(\n",
        "    data.build(engs, fras), try_gpu(), data.num_steps)\n",
        "for en, fr, p in zip(engs, fras, preds):\n",
        "    translation = []\n",
        "    for token in data.tgt_vocab.to_tokens(p):\n",
        "        if token == '<eos>':\n",
        "            break\n",
        "        translation.append(token)\n",
        "    print(f'{en} => {translation}, bleu,'\n",
        "          f'{bleu(\" \".join(translation), fr, k=2):.3f}')"
      ],
      "metadata": {
        "id": "boKNX_ON-Srf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc7e369-cb59-4a0d-fe33-2de4fdd12cd6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go . => ['va', '!'], bleu,1.000\n",
            "i lost . => [\"j'ai\", 'perdu', '.'], bleu,1.000\n",
            "he's calm . => ['il', 'est', 'mouillÃ©', '.'], bleu,0.658\n",
            "i'm home . => ['je', 'suis', 'chez', 'moi', '.'], bleu,1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large language model\n",
        "\n",
        "A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. Based on language models, LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.\n",
        "\n",
        "# Generative pre-trained transformer (GPT)\n",
        "\n",
        "Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content.\n",
        "\n",
        "![Full_GPT_architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Full_GPT_architecture.svg/800px-Full_GPT_architecture.svg.png)\n",
        "\n",
        "## GeLU\n",
        "\n",
        "$$g=0.5x\\Bigg(1+\\frac{2}{\\sqrt{\\pi}}\\int_0^{\\frac{x}{\\sqrt{2}}}e^{-t^2}dt\\Bigg)$$\n",
        "\n",
        "![Activation_gelu](https://upload.wikimedia.org/wikipedia/commons/4/4e/Activation_gelu.png)"
      ],
      "metadata": {
        "id": "5HBXoilNTD85"
      }
    }
  ]
}